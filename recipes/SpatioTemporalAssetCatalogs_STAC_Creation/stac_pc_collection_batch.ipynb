{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unittest\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import pystac\n",
    "import glob\n",
    "import sys\n",
    "from os import path\n",
    "import pdal\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pystac.asset import Asset\n",
    "from pystac.errors import ExtensionTypeError, RequiredPropertyMissing, STACError\n",
    "from pystac.extensions.pointcloud import Schema, SchemaType, AssetPointcloudExtension, PhenomenologyType, PointcloudExtension, Statistic\n",
    "from pystac.extensions.classification import ItemClassificationExtension,AssetClassificationExtension, Classification\n",
    "from pystac.summaries import RangeSummary\n",
    "import logging\n",
    "import constants\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import Polygon, mapping, shape\n",
    "from tempfile import TemporaryDirectory\n",
    "from pyproj import Transformer\n",
    "from botocore.config import Config\n",
    "import pandas as pd \n",
    "import re\n",
    "import json \n",
    "import gc\n",
    "\n",
    "config = Config(signature_version='s3v4')\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables \n",
    "# prefixes for saved stac jsons\n",
    "\n",
    "output_location=r'/stac/output/pc/'\n",
    "\n",
    "json_out=Path(fr\"{output_location}jsons\")\n",
    "# thumbnail_out=fr\"{output_location}thumbnails\"\n",
    "\n",
    "# Object Storage where point clouds are stored \n",
    "catalog_name='BC_STAC'\n",
    "collection_name='PC'\n",
    "public_asset_loc=r'https://nrs.objectstore.gov.bc.ca/gdwuts'\n",
    "\n",
    "obj_type=r'pointcloud' #the type of asset DEM, DSM, CRH, Landcover ect\n",
    "file_extension='.laz'\n",
    "object_store=r\"/mnt/r_drive\"\n",
    "map_area=\"092\"\n",
    "subdivision=['o']\n",
    "# tiles=[18] #rest of test area with DEMS 19,20,28,29,30\n",
    "# root_directory = Path(fr\"{object_store}/{map_area}\") #root_directory = Path(fr\"{object_store}/{map_area}/{map_area}{subdivision}\")\n",
    "root_directories = [Path(f\"{object_store}/{map_area}/{map_area}{subdiv}\") for subdiv in subdivision]\n",
    "intesity_img_dir=Path(f\"{object_store}/intensity_plots/{map_area}/thumbs\")\n",
    "metadata_loc=fr\"{object_store}/metadata/laszy_json\"\n",
    "accuracy_loc=fr\"{object_store}/metadata/accuracy_rpts\"\n",
    "# location for STAC jsons in s3, used to create permanent s3 links and the base link for all collections and items from this script\n",
    "collection_loc=rf\"{catalog_name}/{obj_type}\"\n",
    "\n",
    "public_asset_intensity= rf\"{public_asset_loc}/intensity_plots/{map_area}/thumbs\"\n",
    "\n",
    "\n",
    "# ASPRS Classification Mapping\n",
    "asprs_classes = {\n",
    "    0: \"Never Classified\",\n",
    "    1: \"Unclassified\",\n",
    "    2: \"Ground\",\n",
    "    3: \"Low Vegetation\",\n",
    "    4: \"Medium Vegetation\",\n",
    "    5: \"High Vegetation\",\n",
    "    6: \"Building\",\n",
    "    7: \"Low Noise\",\n",
    "    8: \"Model Key-point\",\n",
    "    9: \"Water\",\n",
    "    10: \"Rail\",\n",
    "    11: \"Road Surface\",\n",
    "    12: \"Overlap Points\",\n",
    "    13: \"Wire - Guard (Shield)\",\n",
    "    14: \"Wire - Conductor (Transmission)\",\n",
    "    15: \"Transmission Tower\",\n",
    "    16: \"Wire-structure Connector\",\n",
    "    17: \"Bridge Deck\",\n",
    "    18: \"High Noise\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set up s3 connection\n",
    "\n",
    "# use third party object storage to create an S3 Client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=constants.AWS_S3_ENDPOINT,\n",
    "    aws_access_key_id=constants.AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=constants.AWS_SECRET_ACCESS_KEY,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "bucket = constants.AWS_S3_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Functions for STAC PC data\n",
    "\n",
    "def capture_date(pdalinfo):\n",
    "    import datetime\n",
    "    year = pdalinfo['creation_year']\n",
    "    day = pdalinfo['creation_doy']\n",
    "    date = datetime.datetime(int(year), 1, 1) + datetime.timedelta(int(day) - 1 if int(day) > 1 else int(day))\n",
    "    return date.isoformat()+'Z'\n",
    "\n",
    "def convertGeometry(geom, srs):\n",
    "    from osgeo import ogr\n",
    "    from osgeo import osr\n",
    "    in_ref = osr.SpatialReference()\n",
    "    in_ref.SetFromUserInput(srs)\n",
    "    out_ref = osr.SpatialReference()\n",
    "    out_ref.SetFromUserInput('EPSG:4326')\n",
    "\n",
    "    g = ogr.CreateGeometryFromJson(json.dumps(geom))\n",
    "    g.AssignSpatialReference(in_ref)\n",
    "    g.TransformTo(out_ref)\n",
    "    return json.loads(g.ExportToJson())\n",
    "\n",
    "\n",
    "def convertBBox(obj):\n",
    "    output = []\n",
    "    output.append(float(obj['minx']))\n",
    "    output.append(float(obj['miny']))\n",
    "    # output.append(float(obj['minz']))\n",
    "    output.append(float(obj['maxx']))\n",
    "    output.append(float(obj['maxy']))\n",
    "    # output.append(float(obj['maxz']))\n",
    "    return output\n",
    "\n",
    "#functions for s3\n",
    "def create_url(bucket_name: str,\n",
    "            object_name: str):\n",
    " \n",
    "    \"\"\"\n",
    "           \n",
    "    This function takes a bucket name, an object name, and an expiration time (in seconds) and generates a URL download link for the object.\n",
    "\n",
    "    Arguments:\n",
    "        bucket_name: String of name of the bucket\n",
    "        object_name: Name of the object (key) that the URL will be pointed to\n",
    "\n",
    "    Returns:\n",
    "        Link of output (object download) URL\n",
    "        \n",
    "    Raises: \n",
    "        Exceptions raised will display an error message and be logged in the export.log file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if r':443' in constants.AWS_S3_ENDPOINT:\n",
    "            endpoint=constants.AWS_S3_ENDPOINT.split(':')\n",
    "            endpoint=fr\"{endpoint[0]}:{endpoint[1]}\"\n",
    "        else:\n",
    "            endpoint=constants.AWS_S3_ENDPOINT\n",
    "        response=os.path.join(endpoint,bucket_name,object_name)\n",
    "    except ClientError as e:\n",
    "        logging.info(e)\n",
    "        return None\n",
    "    return response\n",
    "\n",
    "def set_permissions(bucket_name: str,\n",
    "                    object_name: str,\n",
    "                    permissions='public-read'):\n",
    "    \"\"\"\n",
    "    This function takes a bucket name, an object name, and a permissions value (specified below) and sets the object's permissions to the value given.\n",
    "\n",
    "    Arguments:\n",
    "        bucket_name: String of name of the bucket\n",
    "        object_name: Name of the object (key) that the URL will be pointed to\n",
    "        permissions: If not specified, the permissions will default to 'public-read'. Otherwise, permissions can be found below:\n",
    "        'private'|'public-read'|'public-read-write'|'authenticated-read'|'aws-exec-read'|'bucket-owner-read'|'bucket-owner-full-control'\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "        \n",
    "    Raises: \n",
    "        Exceptions raised will display an error message and be logged in the export.log file \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = s3_client.put_object_acl(ACL=permissions, Bucket=bucket_name, Key=object_name)\n",
    "        logging.info(f'Set permissions on {object_name} success, set to {permissions}')\n",
    "    except Exception as e:\n",
    "        logging.info(f'Error when setting permission: double check permission: {permissions}. Refer to help(set_permissions) for documentation.')\n",
    "        logging.info(e)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def extract_dates(file_path):\n",
    "    file_path = str(file_path)\n",
    "    year_match = re.search(r'/(\\d{4})/', file_path)\n",
    "    year = year_match.group(1) if year_match else None\n",
    "    date_match = re.findall(r'_(\\d{8})', file_path)\n",
    "    \n",
    "    if len(date_match) == 2:\n",
    "        start_date = datetime.strptime(date_match[0], \"%Y%m%d\")\n",
    "        end_date = datetime.strptime(date_match[1], \"%Y%m%d\")\n",
    "    elif len(date_match) == 1:\n",
    "        start_date = datetime.strptime(date_match[0], \"%Y%m%d\")\n",
    "        end_date = start_date\n",
    "    elif year:\n",
    "        start_date = datetime.strptime(year, \"%Y\")\n",
    "        end_date = start_date\n",
    "    else:\n",
    "        start_date, end_date = None, None\n",
    "\n",
    "    return start_date, end_date\n",
    "\n",
    "def find_matching_intesity_jpg(directory: str, item_name: str) -> str | None:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(f\"{item_name}_\") and filename.endswith((\"_grid.jpg\", \"_grid.jpeg\")):\n",
    "            return os.path.join(directory, filename)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create base url for all json hrefs \n",
    "base_url=create_url(bucket, collection_loc)\n",
    "print(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create STAC Catalog \n",
    "#https://pystac.readthedocs.io/en/stable/api/catalog.html\n",
    "\n",
    "url=create_url(bucket, os.path.join(catalog_name,f\"{catalog_name}_catalog.json\"))\n",
    "\n",
    "catalog = pystac.Catalog(id=catalog_name,\n",
    "                        description='This catalog is contains Open Data from the Province of BC',\n",
    "                        stac_extensions=None, #if not none list of extension\n",
    "                        extra_fields = None, #if not none dictionary\n",
    "                        href=url,\n",
    "                        catalog_type='ABSOLUTE_PUBLISHED', # https://pystac.readthedocs.io/en/stable/api/pystac.html#pystac.CatalogType\n",
    "                        ) # https://pystac.readthedocs.io/en/stable/api/layout.html#pystac.layout.BestPracticesLayoutStrategy    https://github.com/radiantearth/stac-spec/blob/v1.1.0/best-practices.md#catalog-layout\n",
    "                        \n",
    "\n",
    "print(json.dumps(catalog.to_dict(), indent=4))\n",
    "\n",
    "\n",
    "print(F\"THIS IS THE URL {url}   !!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_files = []\n",
    "for root_directory in root_directories:\n",
    "    laz_files.extend(root_directory.rglob(f\"{obj_type.lower()}/*{file_extension}\"))\n",
    "\n",
    "print(f\"{len(laz_files)} {file_extension} files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for existing files in output folder \n",
    "\n",
    "\n",
    "laz_files_dict = {os.path.basename(f).rsplit('.', 1)[0]: f for f in laz_files}\n",
    "\n",
    "for root, dirs, files in os.walk(json_out):\n",
    "    for d in dirs:\n",
    "        if d in laz_files_dict:\n",
    "            laz_files.remove(laz_files_dict[d])\n",
    "print(len(laz_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up pdal processing to create json structures \n",
    "batch_size = 25\n",
    "item_count=0\n",
    "total_files = len(laz_files)\n",
    "size_limit = 230 * 1024 * 1024  # 230 MB in bytes\n",
    "while item_count < total_files:\n",
    "    batch = laz_files[item_count:item_count + batch_size]\n",
    "    \n",
    "    for filename in batch:\n",
    "        \n",
    "        try:\n",
    "            item_count=item_count+1\n",
    "            asset_suffix=str(filename).split('drive')[-1]\n",
    "            asset_suffix=asset_suffix.split(file_extension)[0]  \n",
    "            \n",
    "            asset_file=fr\"{public_asset_loc}{asset_suffix}\"\n",
    "            asset_href=fr\"{asset_file}{file_extension}\"\n",
    "        \n",
    "            item_name=str(filename).split('/')[-1]\n",
    "            item_name=item_name.split(file_extension)[0]\n",
    "            asset_name=fr\"{item_name}_{file_extension.split('.')[1]}\"\n",
    "\n",
    "\n",
    "            item_href=f\"{base_url}/{item_name}/{item_name}.json\"\n",
    "            # thumbnail_file=f\"{base_url}/{item_name}/{item_name}.png\"\n",
    "            out_filename = str(f\"{json_out}/{item_name}.json\")\n",
    "            \n",
    "            print(item_name)\n",
    "            \n",
    "            # pdal info --all call references hexbin, stats, and info filters\n",
    "            r = pdal.Reader.las(filename) #our laz files are not Cloud Optimized Point Clouds (COPC)\n",
    "            hb = pdal.Filter.hexbin()\n",
    "            s = pdal.Filter.stats()\n",
    "            i = pdal.Filter.info()\n",
    "            # c = pdal.Filter.range(limits=\"Classification[1:255]\") # classifications removed for time being keeps crashing on larger files \n",
    "            gt=pdal.Filter.gpstimeconvert(conversion=\"gt2gst\")\n",
    "\n",
    "            pipeline: pdal.Pipeline = r | hb | s | i | gt #| c \n",
    "\n",
    "            count = pipeline.execute()\n",
    "            \n",
    "            logging.info('PDAL pipline completed')\n",
    "                        \n",
    "            boundary = pipeline.metadata['metadata'][hb.type]\n",
    "            stats = pipeline.metadata['metadata'][s.type]\n",
    "            info = pipeline.metadata['metadata'][i.type]\n",
    "            copc = pipeline.metadata['metadata'][r.type]\n",
    "            # point_class=pipeline.metadata['metadata'][c.type]\n",
    "            gps_times=pipeline.metadata['metadata'][gt.type]\n",
    "            \n",
    "            gps_epoch = datetime(1980, 1, 6)\n",
    "            leap_seconds = 18\n",
    "            \n",
    "            if not gps_times:\n",
    "                print(\"No GPS time data found in file. pulling from metadata json\")\n",
    "                with open(fr\"{metadata_loc}/{item_name}.json\", \"r\") as f:\n",
    "                    metadata_fromfile = json.load(f)\n",
    "                start_date = metadata_fromfile[\"point_records\"].get(\"date_start\", \"N/A\")\n",
    "                start_date = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n",
    "                end_date = metadata_fromfile[\"point_records\"].get(\"date_end\", \"N/A\")\n",
    "                end_date = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n",
    "                \n",
    "            else:\n",
    "                # Convert GPS time to UTC (subtract leap seconds)\n",
    "                utc_times = [gps_epoch + timedelta(seconds=float(t) - leap_seconds) for t in gps_times]\n",
    "                start_date = min(utc_times)\n",
    "                end_date = max(utc_times)    \n",
    "            \n",
    "            \n",
    "            # # Extract filtered point data\n",
    "            # if len(pipeline.arrays) > 0:\n",
    "            #     classified_points = pipeline.arrays[0]  \n",
    "            #     df = pd.DataFrame(classified_points)  \n",
    "\n",
    "                \n",
    "            #     classification_counts = df[\"Classification\"].value_counts().reset_index()\n",
    "            #     classification_counts.columns = [\"value\", \"count\"]\n",
    "\n",
    "            #     # Map classification values to ASPRS class names\n",
    "            #     classification_counts[\"name\"] = classification_counts[\"value\"].map(asprs_classes).fillna(\"User-defined/Reserved\")\n",
    "                \n",
    "            #     #get gps time and convert to human readable \n",
    "                \n",
    "            #     # gps_times  = pipeline.arrays[0][\"GpsTime\"] \n",
    "            #     # gps_epoch = datetime(1980, 1, 6, 0, 0, 0)\n",
    "            #     # # Approximate leap seconds (PDAL doesn't apply them automatically)\n",
    "            #     # leap_seconds = 18  # As of 2024\n",
    "            #     # utc_times = [gps_epoch + timedelta(seconds=float(t) - leap_seconds) for t in gps_times]\n",
    "                \n",
    "            #     # Print or store results\n",
    "            #     print(f\"Classification Counts for {filename}:\")\n",
    "            #     print(classification_counts, \"\\n\")\n",
    "            # else:\n",
    "            #     logging.warning(f\"No classified points found in {filename}\")\n",
    "\n",
    "            output = {}\n",
    "\n",
    "            try:\n",
    "                output['geometry'] = convertGeometry(\n",
    "                    boundary['boundary_json'],\n",
    "                    copc['comp_spatialreference']\n",
    "                )\n",
    "            except KeyError:\n",
    "                output['geometry'] = stats['bbox']['EPSG:4326']['boundary']\n",
    "\n",
    "            output['bbox'] = convertBBox(stats['bbox']['EPSG:4326']['bbox'])\n",
    "            output['id'] = path.basename(asset_file)\n",
    "            output['type'] = 'Feature'\n",
    "\n",
    "            assets = {'data': {'href': asset_file}}\n",
    "            properties = {}\n",
    "\n",
    "            properties['pc:schemas'] = info['schema']['dimensions']\n",
    "            properties['pc:statistics'] = stats['statistic']\n",
    "            properties['title'] = \"LiDAR BC\"\n",
    "            properties['providers'] = [\n",
    "                {\n",
    "                    \"name\": \"LidarBC\",\n",
    "                    \"description\": \"LidarBC is an initiative to provide open public access to Light Detection and Ranging data (lidar) and associated datasets collected by the Government of British Columbia\",\n",
    "                    \"roles\": [\n",
    "                    \"producer\",\n",
    "                    ],\n",
    "                    \"url\": \"https://lidar.gov.bc.ca/\"\n",
    "                }\n",
    "            ]\n",
    "            properties['pc:type'] = 'lidar' # eopc, lidar, radar, sonar\n",
    "            try:\n",
    "                properties['pc:density'] = boundary['avg_pt_per_sq_unit']\n",
    "            except KeyError:\n",
    "                properties['pc:density'] = 0\n",
    "            properties['pc:count'] = count\n",
    "\n",
    "            properties['datetime'] = capture_date(copc)\n",
    "\n",
    "            output['properties'] = properties\n",
    "            output['assets'] = assets\n",
    "            output['stac_extensions'] = ['https://stac-extensions.github.io/pointcloud/v1.0.0/schema.json']\n",
    "            output['stac_version'] = '1.0.0'\n",
    "            \n",
    "            self_link = {'rel':'alternate',\"href\":item_href} #need to change to s3 bucket public link to json\n",
    "            lic_link = {'rel':'license',\"href\":'https://www2.gov.bc.ca/gov/content/data/policy-standards/open-data/open-government-licence-bc'} #need to change\n",
    "            output['links'] = [self_link, lic_link]\n",
    "\n",
    "            # with open(out_filename, 'w') as laz_out:\n",
    "            #     laz_out.write(json.dumps(output, sort_keys=True, indent=2,\n",
    "            #                                 separators=(',', ': ')))\n",
    "            \n",
    "            \n",
    "            logging.info('json dumps PC info to local')\n",
    "            \n",
    "            # laz_file_name = filename.stem\n",
    "            # corresponding_json = out_filename\n",
    "            logging.info('start STAC item creation')\n",
    "            # Create and validate STAC item\n",
    "            \n",
    "            item = pystac.Item(\n",
    "                id=item_name,\n",
    "                geometry=output[\"geometry\"],\n",
    "                bbox=output[\"bbox\"],\n",
    "                datetime=start_date,\n",
    "                properties=output[\"properties\"],\n",
    "                start_datetime=start_date,\n",
    "                end_datetime=end_date,\n",
    "                href= item_href\n",
    "            )\n",
    "            \n",
    "            item.common_metadata.created=datetime.now(tz=timezone.utc)\n",
    "            item.common_metadata.updated=None\n",
    "\n",
    "            \n",
    "            logging.info('add assets')\n",
    "            # Add the data asset\n",
    "            item.add_asset(\n",
    "                key=asset_name,\n",
    "                asset=pystac.Asset(\n",
    "                    href=str(asset_href),\n",
    "                    media_type=\"application/vnd.laszip+copc\"\n",
    "                )\n",
    "            )    \n",
    "\n",
    "            #find intenisty jpg \n",
    "            intesity_jpg=find_matching_intesity_jpg(intesity_img_dir,item_name)\n",
    "            \n",
    "            if intesity_jpg is not None:\n",
    "                \n",
    "                intesity_jpg=os.path.basename(intesity_jpg)\n",
    "                public_asset_intensity= rf\"{public_asset_loc}/intensity_plots/{map_area}/thumbs/{intesity_jpg}\"\n",
    "            \n",
    "            \n",
    "                #add thumbnail\n",
    "                item.add_asset(\n",
    "                key=f\"{item_name}_intensity_grid\",\n",
    "                asset=pystac.Asset(\n",
    "                    href=public_asset_intensity,\n",
    "                    media_type=pystac.MediaType.PNG,\n",
    "                    roles=['thumbnail','overview','visual']\n",
    "                    )\n",
    "                )\n",
    "            #add links\n",
    "            # \"self\" link\n",
    "            self_link = pystac.Link(\n",
    "                rel='alternate',\n",
    "                target=item_href\n",
    "            )\n",
    "\n",
    "            # \"license\" link\n",
    "            lic_link = pystac.Link(\n",
    "                rel=\"license\",\n",
    "                target=\"https://www2.gov.bc.ca/gov/content/data/policy-standards/open-data/open-government-licence-bc\"\n",
    "            )\n",
    "\n",
    "            # Add them to the item\n",
    "            item.links.extend([self_link, lic_link])\n",
    "\n",
    "            output[\"links\"] = [\n",
    "                {\"rel\": 'alternate', \"href\": item_href},\n",
    "                {\"rel\": \"license\", \"href\": \"https://www2.gov.bc.ca/gov/content/data/policy-standards/open-data/open-government-licence-bc\"}\n",
    "            ]\n",
    "\n",
    "            if 'dimensions' in output:\n",
    "                print('key exists')\n",
    "                \n",
    "            #set point cloud extension\n",
    "            pc_ext = PointcloudExtension.ext(item, add_if_missing=True)\n",
    "\n",
    "            # Number of points\n",
    "            pc_ext.count = count\n",
    "\n",
    "            # Type of phenomenology (LiDAR, radar, etc.)\n",
    "            pc_ext.type = PhenomenologyType.LIDAR  \n",
    "\n",
    "            # If you have an encoding (e.g., LAZ, COPC)\n",
    "            pc_ext.encoding = \"LAZ\"\n",
    "\n",
    "            # Density (points per square unit)\n",
    "            try:\n",
    "                pc_ext.density = output['properties']['pc:density']\n",
    "            except KeyError:\n",
    "                pc_ext.density = 0\n",
    "\n",
    "            # --------------- SCHEMA ---------------\n",
    "\n",
    "            dimension_dicts = output['properties']['pc:schemas']  \n",
    "\n",
    "            schemas = []\n",
    "            for d in dimension_dicts:\n",
    "                stype = {\n",
    "                    \"floating\": SchemaType.FLOATING,\n",
    "                    \"unsigned\": SchemaType.UNSIGNED,\n",
    "                    \"signed\":   SchemaType.SIGNED\n",
    "                }.get(d[\"type\"], SchemaType.FLOATING)\n",
    "\n",
    "                s = Schema({\n",
    "                    \"name\": d[\"name\"],\n",
    "                    \"size\": d[\"size\"],\n",
    "                    \"type\": stype.value \n",
    "                })\n",
    "                schemas.append(s)\n",
    "\n",
    "            pc_ext.schemas = [s for s in schemas]  \n",
    "\n",
    "            # -------------- STATISTICS --------------\n",
    "\n",
    "            stat_dicts = output[\"properties\"][\"pc:statistics\"]\n",
    "            statistics = []\n",
    "\n",
    "            for s in stat_dicts:\n",
    "                # Build a dict\n",
    "                stat_data = {\n",
    "                    \"name\": s[\"name\"],\n",
    "                    \"average\": s.get(\"average\"),\n",
    "                    \"count\": s.get(\"count\"),\n",
    "                    \"minimum\": s.get(\"minimum\"),\n",
    "                    \"maximum\": s.get(\"maximum\"),\n",
    "                    \"stddev\": s.get(\"stddev\")\n",
    "                }\n",
    "\n",
    "                # Wrap that dict with Statistic(...)\n",
    "                statistics.append(Statistic(stat_data))\n",
    "\n",
    "            pc_ext.statistics = statistics\n",
    "            \n",
    "            #classificatiosn extension removed for time being \n",
    "            \n",
    "            # classification_ext  = ItemClassificationExtension.ext(item, add_if_missing=True)\n",
    "\n",
    "            # # Define classifications\n",
    "            # classifications = []\n",
    "            # for _, row in classification_counts.iterrows():\n",
    "            #     classification = Classification.create(\n",
    "            #         value=int(row[\"value\"]),\n",
    "            #         name=row[\"name\"].replace(\" \", \"_\"),\n",
    "            #         count=int(row[\"count\"])\n",
    "            #     )\n",
    "            #     classifications.append(classification)\n",
    "\n",
    "            # # Assign classifications to the asset\n",
    "            # classification_ext.classes = classifications\n",
    "\n",
    "            item.validate()\n",
    "\n",
    "            item_path = out_filename\n",
    "            # item.save_object(include_self_link= False, dest_href=str(item_path)) #need to change to s3 bucket and make link public \n",
    "            catalog.add_item(item)\n",
    "            print(f'items processed: {item_count}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            break\n",
    "    #get collection bounds\n",
    "    unioned_footprint = None\n",
    "    datetime_list=[]\n",
    "    item_list=[]\n",
    "    for item in catalog.get_all_items():\n",
    "        datetime_list.append(item.datetime)\n",
    "        item_list.append(item)\n",
    "        footprint = item.geometry\n",
    "    # Convert the footprint geometry to a Shapely shape\n",
    "        footprint_shape = shape(footprint)\n",
    "        \n",
    "        # Perform union operation\n",
    "        if unioned_footprint is None:\n",
    "            # If unioned_footprint is None (first iteration), initialize it with the first footprint\n",
    "            unioned_footprint = footprint_shape\n",
    "        else:\n",
    "            # Otherwise, perform union with the current footprint\n",
    "            unioned_footprint = unioned_footprint.union(footprint_shape)\n",
    "\n",
    "    collection_bbox = list(unioned_footprint.bounds)\n",
    "    spatial_extent = pystac.SpatialExtent(bboxes=[collection_bbox])\n",
    "    \n",
    "    datetime_list = [\n",
    "        dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt.astimezone(timezone.utc)\n",
    "        for dt in datetime_list\n",
    "    ]\n",
    "\n",
    "    collection_interval = sorted(datetime_list)\n",
    "    temporal_extent = pystac.TemporalExtent(intervals=[collection_interval])\n",
    "    collection_extent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n",
    "    \n",
    "    # colection_id='Point-Cloud-Collection-Test'\n",
    "    collection_href=os.path.join(base_url,f\"{collection_name}.json\")\n",
    "    print(collection_href)\n",
    "    collection = pystac.Collection(id=obj_type,\n",
    "                                description='LiDAR Point Clouds for British Columbia',\n",
    "                                extent=collection_extent,\n",
    "                                title=obj_type,\n",
    "                                href=collection_href,\n",
    "                                license='Apache-2.0')\n",
    "\n",
    "\n",
    "    url=create_url(bucket, os.path.join(collection_name,f\"{collection_name}.json\"))\n",
    "    print(F\"THIS IS THE URL {url}   !!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    \n",
    "    collection.add_items(item_list)\n",
    "    catalog.add_child(collection)\n",
    "    print(\"Catalog HREF: \", catalog.get_self_href())\n",
    "    print(\"Collection HREF:\", collection.get_self_href())\n",
    "    print(\"Item HREF: \", item.get_self_href())\n",
    "    \n",
    "    catalog.save(catalog_type=pystac.CatalogType.ABSOLUTE_PUBLISHED, dest_href=fr\"{json_out}\")\n",
    "            \n",
    "\n",
    "\n",
    "    print(f\"Processed {item_count}/{total_files} files.\")\n",
    "\n",
    "print(\"All files processed.\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stac_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
