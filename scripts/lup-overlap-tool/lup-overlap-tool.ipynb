{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUP Overlap Tool\n",
    "\n",
    "This was developed by North Ross for use with Tahltan, Blueberry River FN and Doig River FN LUP planning analysis. The example here is for the Doig analysis.\n",
    "\n",
    "Developed with Python version: 3.13.1;\n",
    "\n",
    "Geopandas 1.0.1;\n",
    "\n",
    "Pandas 2.2.3\n",
    "\n",
    "The optional last section requires xlsxwriter 3.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from shapely import geometry, wkt\n",
    "from json import load\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "import oracledb\n",
    "\n",
    "import logging\n",
    "#set up logging \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "debug=logging.debug\n",
    "info=logging.info\n",
    "warning=logging.warning\n",
    "error=logging.error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preform an Identity operation on the relevant layers in ArcPro. \n",
    "\n",
    "Note - Geopandas might run out of memory when working on data with lots of verticies. Using the .simplify() method might help, or you can do all the union operations in ArcPro, then read the output and start in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doig Layers\n",
    "- Doig LPP\n",
    "- Enhanced Planning Areas\n",
    "- Doig Management Zones\n",
    "- Consensus Protection Zones\n",
    "- Enhanced Management\n",
    "\n",
    "### FN Planning Zones\n",
    "- LRMPs\n",
    "- LRMP related Non Legal Planning Features\n",
    "- Priority WMBs\n",
    "- HV1 Zones\n",
    "- Traplines\n",
    "- PNG Zones\n",
    "- HRFN AMPP\n",
    "- HRFN EMC\n",
    "- South Peace Strategy\n",
    "- Saulteau Enhanced Management\n",
    "- Liard Water and Land Stewardship Forum\n",
    "- Liard Watershed Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BCGW connection from config file\n",
    "\n",
    "# Either edit this to add your BCGW user and pass, or connect to it a separate config file as I've done here\n",
    "with open('bcgw-config.json') as f:\n",
    "    config = load(f)\n",
    "    class connection:\n",
    "        user_nm = config['user_nm']\n",
    "        bcgw_pass = config['bcgw_pass']\n",
    "        host_nm= \"bcgw.bcgov\"\n",
    "        service_nm= \"idwprod1.bcgov\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input definition excel sheets which list the output rows and columns.\n",
    "define_xlsx = 'OutputRowsAndCols_2025-04-17.xlsx'\n",
    "\n",
    "# Define the output file - this can be read to avoid re-running the intersect analysis portion if you need to restart the kernel\n",
    "outdir = 'output-example'\n",
    "output_union_file = outdir + '\\\\union_data.gpkg'\n",
    "output_union_layer = f'union_{(date.today().strftime(\"%Y-%m-%d\"))}'\n",
    "\n",
    "define_inputs_df = pd.read_excel(define_xlsx, sheet_name='Inputs')\n",
    "define_rows_df = pd.read_excel(define_xlsx, sheet_name='Rows')\n",
    "define_columns_df = pd.read_excel(define_xlsx, sheet_name='Columns')\n",
    "\n",
    "conn=oracledb.connect(user=connection.user_nm, password= connection.bcgw_pass, host=connection.host_nm, port=1521,\n",
    "                        service_name=connection.service_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def get_bcgw_layer(connection, table, clause, aoi_geom, do_aoi_select, col_list):\n",
    "    conn=oracledb.connect(user=connection.user_nm, password= connection.bcgw_pass, host=connection.host_nm, port=1521,\n",
    "                            service_name=connection.service_nm)\n",
    "    #create cursor and excecute sql\n",
    "    with conn.cursor() as cursor:\n",
    "        \n",
    "        # First, find the primary geometry column\n",
    "        cursor.execute(f\"SELECT * FROM {table}\")\n",
    "        for col in cursor.description:\n",
    "            if col.type.name == 'SDO_GEOMETRY':\n",
    "                info(f\"Geometry column for {table}: {col.name}\")\n",
    "                geometry_column = col.name\n",
    "        \n",
    "        # Next, do the actual query\n",
    "        # add a spatial selector to the end of the query after the clause (if exists)\n",
    "        aoi_clause = f\"\"\"SDO_ANYINTERACT ({geometry_column},\n",
    "                            SDO_GEOMETRY(2003, 3005, NULL,\n",
    "                                SDO_ELEM_INFO_ARRAY(1,1003,3),\n",
    "                                SDO_ORDINATE_ARRAY({\",\".join(map(str, aoi_geom.bounds))}) \n",
    "                            )\n",
    "                        ) = 'TRUE'\"\"\"\n",
    "        if do_aoi_select == 'Yes':\n",
    "            if clause:\n",
    "                clause = f'WHERE {clause} AND {aoi_clause}'\n",
    "            else:\n",
    "                clause = 'WHERE ' + aoi_clause\n",
    "        else:\n",
    "            if clause:\n",
    "                clause = 'WHERE ' + clause\n",
    "                \n",
    "        # append geometry col to col list:\n",
    "        \n",
    "        query = f\"\"\"SELECT SDO_UTIL.TO_WKTGEOMETRY({geometry_column}) AS WKT_LOB, {','.join(col_list)} \n",
    "                    FROM {table} {clause}\n",
    "                \"\"\"\n",
    "        info(f\"executing query {query}\")\n",
    "        cursor.execute(query)\n",
    "        columns = [col[0] for col in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # read to df and convert to shapely geometry\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        df['wkt'] = df['WKT_LOB'].apply(oracledb.LOB.read) # read oracle LOB object to WKT string\n",
    "        df['geometry'] = df['wkt'].apply(wkt.loads) # read WKT string to shapely geometry with shapely.wkt.loads()\n",
    "        df = df.drop(columns=['WKT_LOB', 'wkt'])\n",
    "        \n",
    "        info(f\"Returned {len(df)} records\")\n",
    "    conn.close()\n",
    "    # make wkt readable by gpd by using a geoseries and the function from_wkt\n",
    "    # df['geometry']=gpd.GeoSeries.from_wkt(df['geometry'])\n",
    "    gdf=gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    # set crs to bc albers\n",
    "    gdf = gdf.set_crs(3005, allow_override=True)\n",
    "    return gdf\n",
    "\n",
    "def read_input(src, rename_dict, clause = None, aoi_geom = None, aoi_select = None, clip = None):\n",
    "    # clean inputs\n",
    "    if type(clause) != str or clause == '':\n",
    "        clause = None\n",
    "    if type(clip) != str or clip == '':\n",
    "        clip = None\n",
    "    rename_dict = eval(rename_dict)\n",
    "    \n",
    "    # Get BCGW layer as geodataframe\n",
    "    if src[:4] == 'WHSE':\n",
    "        df = get_bcgw_layer(connection, src, clause, aoi_geom, aoi_select, col_list = list(rename_dict.keys()))\n",
    "    \n",
    "    # Get feature class from geodatabase as gdf\n",
    "    elif '.gdb' in src:\n",
    "        gdf_i = src.find('.gdb')\n",
    "        gdb_path = src[:gdf_i+4]\n",
    "        layer = src[gdf_i+5:]\n",
    "        df = gpd.read_file(gdb_path, layer = layer, where=clause)\n",
    "    \n",
    "    # read shp or other\n",
    "    else:\n",
    "        df = gpd.read_file(src, where=clause)\n",
    "    \n",
    "    # Drop fields, rename and dissolve\n",
    "    col_list = list(rename_dict.keys())\n",
    "    col_list.append('geometry')\n",
    "    df = df[col_list]\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    df = df.dissolve(by = list(rename_dict.values())).reset_index()\n",
    "    \n",
    "    # reproject to BC albers (3005)\n",
    "    if df.crs.to_epsg() != 3005:\n",
    "        df = df.to_crs(\"EPSG:3005\")\n",
    "    \n",
    "    # Select only polygons within bounds of AOI (if defined). \n",
    "    if aoi_geom:\n",
    "        df = df[df.intersects(geometry.box(*aoi_geom.bounds))]\n",
    "        \n",
    "        # Clip to AOI geometry if indicated in spreadsheet\n",
    "        if clip != None:\n",
    "            df = gpd.clip(df, aoi_geom)\n",
    "    \n",
    "    # make geometry valid\n",
    "    df['geometry'] = df['geometry'].make_valid()\n",
    "        \n",
    "    # set precision to 0.1m to avoid errors:\n",
    "    df['geometry'] = df['geometry'].set_precision(0.1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Read all inputs to a list    \n",
    "def read_all_inputs_to_list(define_inputs_df, aoi_gdf):\n",
    "    input_gdf_list = []\n",
    "    aoi_geom = aoi_gdf.geometry.union_all()\n",
    "    for i, r, in define_inputs_df.iterrows():\n",
    "        info(f\"reading layer {r['Name']}\")\n",
    "        input_gdf = read_input(r['Source'], r['Rename Dictionary'], r['Clause'], aoi_geom, r['Select AOI'], r['Clip to AOI'])\n",
    "        input_gdf_list.append(input_gdf)\n",
    "    \n",
    "    return input_gdf_list\n",
    "\n",
    "# Preform union of all inputs\n",
    "def union_all_inputs(aoi_gdf, input_gdf_list):\n",
    "    union_gdf = aoi_gdf\n",
    "    for gdf in input_gdf_list[1:]:\n",
    "        info(f\"intersecting {list(gdf.columns)[0]}\")\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
    "                info(\"Successfully intersected\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if 'Topology' in str(e):\n",
    "                    if attempt != 1:\n",
    "                        union_gdf.geometry = union_gdf.geometry.buffer(0)\n",
    "                        gdf.geometry = gdf.geometry.simplify(1) # attempt to simplify new layer to 1m\n",
    "                        gdf.geometry = gdf.geometry.buffer(0) # buffer by 0m - this sometimes fixes the problem\n",
    "                        gdf.geometry = gdf.geometry.make_valid() # fix geometry\n",
    "                        warning(e)\n",
    "                        warning(\"trying again with 1m simplication, 0m buffer and fixing geometry\")\n",
    "                    else:\n",
    "                        error(e)\n",
    "                else:\n",
    "                    error(\"could not intersect\")\n",
    "                    sys.exit()\n",
    "        union_gdf = overlay_gdf\n",
    "        info(f\"output polygons: {len(union_gdf)}\")\n",
    "    return union_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - print outputs for reference\n",
    "define_inputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading layer AOI\n",
      "INFO:root:reading layer DRFN Landscape Planning Pilot\n",
      "INFO:root:reading layer DRFN Management Zones\n",
      "INFO:root:reading layer DRFN Enhanced Planning Areas\n",
      "INFO:root:reading layer DRFN Consensus Protection Zones\n",
      "INFO:root:reading layer BRFN Priority WMB\n",
      "INFO:root:reading layer BRFN HV1\n",
      "c:\\Users\\NROSS\\.conda\\envs\\gpdoracle\\Lib\\site-packages\\pyogrio\\raw.py:198: UserWarning: Measured (M) geometry types are not supported. Original type 'Measured 3D MultiPolygon' is converted to 'MultiPolygon Z'\n",
      "  return ogr_read(\n",
      "INFO:root:reading layer South Peace Strategy Polygon\n",
      "INFO:root:reading layer Liard WLSF\n",
      "INFO:root:reading layer HRFN AMPP\n",
      "INFO:root:reading layer Boreal Caribou RRAs\n",
      "INFO:root:intersecting DRFN_LPP\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 3\n",
      "INFO:root:intersecting DRFN_MgmtZone_Name\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 60 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 15\n",
      "INFO:root:intersecting Doig_EPA_Name\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 177 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 28\n",
      "INFO:root:intersecting Doig_CPZ_Name\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 39 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 37\n",
      "INFO:root:intersecting BRFN_WMB\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 14 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 61\n",
      "INFO:root:intersecting HV1 Name\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 166 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 112\n",
      "INFO:root:intersecting South Peace Strategy\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 231 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 125\n",
      "INFO:root:intersecting Liard_WLSF\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 66 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 130\n",
      "INFO:root:intersecting HRFN_AMPP\n",
      "C:\\Users\\NROSS\\AppData\\Local\\Temp\\ipykernel_36276\\634562243.py:125: UserWarning: `keep_geom_type=True` in overlay resulted in 1025 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlay_gdf = gpd.overlay(union_gdf, gdf, how='union')\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 163\n",
      "INFO:root:intersecting BorealRRA\n",
      "INFO:root:Successfully intersected\n",
      "INFO:root:output polygons: 169\n",
      "INFO:pyogrio._io:Created 169 records\n"
     ]
    }
   ],
   "source": [
    "# execute read and union functions\n",
    "aoi_row = define_inputs_df[define_inputs_df['Name'] == 'AOI']\n",
    "\n",
    "aoi_gdf = read_input(\n",
    "                aoi_row['Source'][0],\n",
    "                aoi_row['Rename Dictionary'][0],\n",
    "                aoi_row['Clause'][0],\n",
    "            )\n",
    "input_gdf_list = read_all_inputs_to_list(define_inputs_df, aoi_gdf)\n",
    "df = union_all_inputs(aoi_gdf, input_gdf_list)\n",
    "df['AreaHa'] = df.area / 10000\n",
    "df\n",
    "# save df to file so it can be retrieved\n",
    "df.to_file(output_union_file, layer = output_union_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you skip the overlay part and do it in another software - run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read final polygon after union operations\n",
    "gdb = r\"\\\\<path_to_gdb>\"\n",
    "layer_name = 'Union_Planning_2025_03_12'\n",
    "\n",
    "df = gpd.read_file(gdb, layer=layer_name)\n",
    "\n",
    "# then rename all your fields to match the what's in the input sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any edits to the data required here:\n",
    "\n",
    "# Replace \"GRAZING RESERVE 1\", \"GRAZING RESERVE 2\" etc. with 'GRAZING RESERVE' so that results are combined in output\n",
    "df.replace(regex = r'GRAZING RESERVE \\d', value = 'GRAZING RESERVE', inplace=True)\n",
    "df = df.rename(columns={'RMZName': \"RMZ Name\", \"RMZType\": \"RMZ Type\"})\n",
    "df.loc[\n",
    "    (df['DRFN_MgmtZone_Name'].notnull()) &\n",
    "    ~(df['BRFN_WMB'].notnull()) &\n",
    "    ~(df['HRFN_AMPP'].notnull()) &\n",
    "    ~(df['South Peace Strategy'].notnull()) &\n",
    "    ~(df['Liard_WLSF'].notnull()), \n",
    "   \n",
    "   'DRFN Core'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define functions used in \n",
    "\n",
    "# Prepare the output data frame. This can be exported to an Excel\n",
    "def createOutputDf():\n",
    "    # First define the empty dataframe to be added onto later\n",
    "    outputdf = pd.DataFrame()\n",
    "\n",
    "    # Loop over the rows df\n",
    "    for i, r in define_rows_df.iterrows():\n",
    "        # create a blank \"row dataframe\" that we will add all the columns to. This will be appended to the output df\n",
    "        rdf = pd.DataFrame()\n",
    "        group = r['GroupField']\n",
    "        areaSeries = eval(r['Area'])\n",
    "        # Loop over columns df\n",
    "        for ic, c in define_columns_df.iterrows():\n",
    "            # get some variables from the dict so it's easier to read:\n",
    "            maskSeries = eval(c['Mask'])\n",
    "            cname = c['Name']\n",
    "            \n",
    "            # Create \"Column dataframe\" using a subset of the one defined in the row dictionary.\n",
    "            cdf = areaSeries.loc[maskSeries]\n",
    "            \n",
    "            # Choose the \"Sum Field\". If none is specified it does the Area.\n",
    "            if pd.isnull(c['sumField']):\n",
    "                sumField = 'AreaHa'\n",
    "            else:\n",
    "                sumField = c['sumField']\n",
    "            \n",
    "            # Group the results by designated field\n",
    "            if pd.isnull(group):\n",
    "                # If the group field is empty, sum the entire selection and remove all other fields\n",
    "                cdf = pd.DataFrame([cdf[[sumField]].sum()], index=[\"All\"])\n",
    "            elif ', ' in group:\n",
    "                # if the group has multiple fields, then concat them together\n",
    "                group = group.split(\", \")\n",
    "                try:\n",
    "                    cdf['merged'] = cdf[group[0]].astype('str') + \" - \" + cdf[group[1]].astype('str')\n",
    "                    cdf = cdf[['merged', sumField]].groupby('merged').sum()\n",
    "                except:\n",
    "                    raise Exception(f\"Cannot group by more than two fields. Remove extra commas from: {group} (row {ic+1} in columns).\")\n",
    "            else:\n",
    "                # Remove all columns except group field and area, then preform group by and sum operations.\n",
    "                cdf = cdf[[group, sumField]].groupby(group).sum()\n",
    "        \n",
    "            # Add category (from row dictionary)\n",
    "            cdf['Category'] = r['Category']\n",
    "            \n",
    "            # Set index to a multi index of Category and \"Polygon\" name\n",
    "            cdf = cdf.set_index(['Category', cdf.index.rename('Polygon')])\n",
    "\n",
    "            # Rename the area field to the column name for output\n",
    "            cdf = cdf.rename(columns={'AreaHa': cname})\n",
    "            \n",
    "            # If rdf is empty, set it to the \"cdf\"\n",
    "            if len(rdf) == 0:\n",
    "                rdf = cdf\n",
    "            # otherwise, join the cdf on to the existing \"rdf\"\n",
    "            else:\n",
    "                rdf = rdf.join(cdf, how='outer')\n",
    "        # Add the rdf to the output dataframe\n",
    "        outputdf = pd.concat([outputdf, rdf])\n",
    "    return outputdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Excel with date\n",
    "now = date.today()\n",
    "datestr = now.strftime(\"%Y-%m-%d\")\n",
    "outfile = os.path.join(outdir, f'LUP Analysis Pandas RMZ Planning {datestr}.xlsx')\n",
    "outputdf = createOutputDf()\n",
    "outputdf.style.map(lambda v: \"number-format: #,##0\").to_excel(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding plots to the output excel\n",
    "\n",
    "This optional part of the script takes geopandas plot images of the layers and embedded these in the columns for reference. This works best in cases when you have less than 10 columns with confusing geometry. The way this works is that after the Excel gets exported, this function opens the output XLSX, adds a row at the top and embeds all the PNGs of exported from the Column Dataframes used in the output function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6733\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5454\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5837\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5245\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5951\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 8607\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 8607\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6324\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5559\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 7314\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6531\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6733\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5454\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5837\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5245\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5951\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 8607\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 8607\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6324\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 5559\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 7314\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'tEXt' 41 58\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 111 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 132 6531\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def savePlot(area, name, number, worksheet = \"Sheet1\"):\n",
    "    \"\"\"Save plot of area to png and embed in Excel worksheet above the column\n",
    "\n",
    "    Args:\n",
    "        area (geopandas.GeoDataFrame): Geodataframe of area for this column\n",
    "        name (str): Column name\n",
    "        number (int): Column number (index)\n",
    "        worksheet (str): Name of worksheet to embed the analysis in\n",
    "    \"\"\"\n",
    "    plt.ioff()\n",
    "    \n",
    "    ax = area.dissolve().plot(color='green', legend=True)\n",
    "    # <add other reference layers here>\n",
    "    \n",
    "    # add full AOI\n",
    "    p1 = aoi_gdf.plot(color='none', edgecolor='black', ax=ax)\n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    plotdir = os.path.join(outdir, worksheet)\n",
    "    if not os.path.exists(plotdir):\n",
    "        os.mkdir(plotdir)\n",
    "    path = os.path.join(plotdir, f'{number}_{name}.png')\n",
    "    plt.savefig(path, dpi=60.0, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "# embed plot maps in Excel\n",
    "def embedPlots(xlsx_str, worksheetName = \"Sheet1\"):\n",
    "    \"\"\"Embeds saved png plots into the output Excel in the top row above their relevant column\n",
    "\n",
    "    Args:\n",
    "        xlsx_str (str): Path to Excel file. \n",
    "        worksheetName (str): Name of output worksheet to embed in.\n",
    "    \"\"\"\n",
    "    \n",
    "    workbook = openpyxl.load_workbook(xlsx_str)\n",
    "    if worksheetName:\n",
    "        ws = workbook[worksheetName]\n",
    "    else:\n",
    "        ws = workbook.active\n",
    "\n",
    "    ws.insert_rows(1)\n",
    "    ws.row_dimensions[1].height = 170\n",
    "    for f in os.listdir(os.path.join(outdir, worksheetName)):\n",
    "        if f[-4:] == '.png':\n",
    "            path = os.path.join(outdir, worksheetName, f)\n",
    "            num = int(f.split('_')[0])\n",
    "            col_letter = openpyxl.utils.get_column_letter(num+2)\n",
    "            img = openpyxl.drawing.image.Image(path)\n",
    "            ws.add_image(img, f'{col_letter}1')\n",
    "            ws.column_dimensions[col_letter].width= 16\n",
    "\n",
    "    workbook.save(outfile)\n",
    "\n",
    "# export all plots:\n",
    "for ic, c in define_columns_df.iterrows():\n",
    "    # get some variables from the dict so it's easier to read:\n",
    "    maskSeries = eval(c['Mask'])\n",
    "    cname = c['Name']\n",
    "    # remove non-alphanumeric characters in file name string\n",
    "    cname_outstring = \"\".join(x for x in cname if x.isalnum())\n",
    "    \n",
    "    # get column df and save plot\n",
    "    cdf = df.loc[maskSeries]\n",
    "    savePlot(cdf, cname_outstring, ic+1)\n",
    "    info(f\"Saved plot {ic+1} for {cname_outstring}\")\n",
    "\n",
    "# embed in Excel\n",
    "embedPlots(outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpdoracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
